{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# algorithm-efficiency-complexity\n",
    "> [TABLE OF CONTENTS](https://github.com/E6985/E2746/blob/master/README.md)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a **priori** analysis - complexity measures an algorithms efficiency with respect to **internal** factors such as the time needed to run an algorithm - measures the efficiency of an algorithms design - eliminating the effects of platform-specific implementation details - CPU - compiler design [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- complexity - how do the resource requirements of a program or algorithm scale - ie - what happens as the size of the problem being solved or input dataset gets larger - complexity affects performance but not the other way around - comparing the relative efficiency of algorithms by evaluating their running time complexity on input data of size - n - memory or storage requirements of an algorithm could also be evaluated in this matter - eg - how much longer will an algorithm take to execute if inputting a list of 1000 elements instead of 10 elements [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that algorithms are platform independent - ie - any algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system therefore empirical comparisons of an algorithms complexity are of limited use if we wish to draw general conclusions about the relative performance of different algorithms as the results obtained are highly dependent on the specific platform which is used - need a way to compare the complexity of algorithms that is also platform independent - can analyse complexity **mathematically** [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- can compare algorithms by evaluating their running **time** complexity on input data of size - n - standard methodology developed over the past half century for comparing algorithms - can determine which algorithms scale well to solve problems of a nontrivial size by evaluating the complexity the algorithm in relation to the size - n - of the provided input - typically algorithmic complexity falls into one of a number families - ie - the growth in its execution time with respect to increasing input size - n  is of a certain order - the effect of higher order growth functions becomes more significant as the size - n - of the input set is increased - memory or storage requirements of an algorithm could also be evaluated in this manner [6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
